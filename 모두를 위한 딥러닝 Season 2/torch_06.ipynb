{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"torch_intro_06.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNd/5lnvRsqxtxVHf+CUJpa"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"heQzcnTCbnV6","executionInfo":{"status":"ok","timestamp":1608120987764,"user_tz":-540,"elapsed":1176,"user":{"displayName":"송대선","photoUrl":"","userId":"01162176048652332038"}}},"source":["import numpy as np\n","import torch"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"TXSAXuHeevgD","executionInfo":{"status":"ok","timestamp":1608121034005,"user_tz":-540,"elapsed":1149,"user":{"displayName":"송대선","photoUrl":"","userId":"01162176048652332038"}}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class MultivariableLinearRegressionModel(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.linear = nn.Linear(3,1)\n","  \n","  def forward(self, x):\n","    return self.linear(x)\n","\n","model = MultivariableLinearRegressionModel()\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"EMjfJG9ZcAEa","executionInfo":{"status":"ok","timestamp":1608120872423,"user_tz":-540,"elapsed":1031,"user":{"displayName":"송대선","photoUrl":"","userId":"01162176048652332038"}}},"source":["from torch.utils.data import Dataset\n","\n","class CustomDataset(Dataset):\n","  def __init__(self):\n","    self.x_data = torch.FloatTensor([[73, 80, 75],\n","                                [93, 88, 93],\n","                                [89, 91, 90],\n","                                [96, 98, 100],\n","                                [73, 66, 70]])\n","    self.y_data = torch.FloatTensor([[152], [185], [180], [196], [142]])\n","\n","  def __len__(self):   # 이 데이터셋의 총 데이터 수\n","    return len(self.x_data)\n","\n","  def __getitem__(self, idx): # 어떠한 인덱스 idx를 받았을 때, 그에 상응하는 입출력 데이터를 반환\n","    x = torch.FloatTensor(self.x_data[idx])\n","    y = torch.FloatTensor(self.y_data[idx])\n","\n","    return x, y\n","\n","dataset = CustomDataset()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"6NPLtaY-duhR","executionInfo":{"status":"ok","timestamp":1608120879103,"user_tz":-540,"elapsed":955,"user":{"displayName":"송대선","photoUrl":"","userId":"01162176048652332038"}}},"source":["from torch.utils.data import DataLoader\n","\n","dataloader = DataLoader(\n","    dataset,\n","    batch_size = 2,  # 각 minibatch의 크기\n","    shuffle=True,    # Epoch마다 데이터셋을 섞어서, 데이터가 학습되는 순서를 바꾼다.\n",")"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U9_iY5G9eH3g","executionInfo":{"status":"ok","timestamp":1608121133093,"user_tz":-540,"elapsed":942,"user":{"displayName":"송대선","photoUrl":"","userId":"01162176048652332038"}},"outputId":"3fc72720-03af-479e-d954-3a72091050da"},"source":["nb = 20\n","\n","for epoch in range(1, nb+1):\n","  for batch_idx, samples in enumerate(dataloader):\n","    x_train, y_train = samples\n","\n","    prediction = model(x_train)\n","\n","    cost = F.mse_loss(prediction, y_train)\n","\n","    # 아래 3개는 항상 같이 다닌다.\n","    optimizer.zero_grad() # gradient 초기화\n","    cost.backward()       # gradient 계산\n","    optimizer.step()      # step()으로 개선\n","\n","    print('Epoch: {:4d}/{}, Batch:{}/{}, hypothesis: {}, cost: {:.6f}'.format(epoch, nb, batch_idx+1, len(dataloader), prediction.squeeze().detach(), cost.item()))\n","\n","\n","prediction = model(x_train)\n","print(prediction)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Epoch:    1/20, Batch:1/3, hypothesis: tensor([141.5276, 180.1851]), cost: 0.128691\n","Epoch:    1/20, Batch:2/3, hypothesis: tensor([196.9786, 150.6328]), cost: 1.413371\n","Epoch:    1/20, Batch:3/3, hypothesis: 185.09393310546875, cost: 0.008823\n","Epoch:    2/20, Batch:1/3, hypothesis: tensor([196.9516, 180.2043]), cost: 0.473669\n","Epoch:    2/20, Batch:2/3, hypothesis: tensor([141.3090, 184.7410]), cost: 0.272282\n","Epoch:    2/20, Batch:3/3, hypothesis: 150.5211639404297, cost: 2.186956\n","Epoch:    3/20, Batch:1/3, hypothesis: tensor([180.7038, 197.4953]), cost: 1.365662\n","Epoch:    3/20, Batch:2/3, hypothesis: tensor([141.4898, 150.5558]), cost: 1.172966\n","Epoch:    3/20, Batch:3/3, hypothesis: 185.3760986328125, cost: 0.141450\n","Epoch:    4/20, Batch:1/3, hypothesis: tensor([150.7309, 180.3439]), cost: 0.864482\n","Epoch:    4/20, Batch:2/3, hypothesis: tensor([197.2960, 141.7862]), cost: 0.862675\n","Epoch:    4/20, Batch:3/3, hypothesis: 185.05982971191406, cost: 0.003580\n","Epoch:    5/20, Batch:1/3, hypothesis: tensor([196.9341, 150.5999]), cost: 1.416438\n","Epoch:    5/20, Batch:2/3, hypothesis: tensor([141.5599, 185.0703]), cost: 0.099319\n","Epoch:    5/20, Batch:3/3, hypothesis: 180.2942657470703, cost: 0.086592\n","Epoch:    6/20, Batch:1/3, hypothesis: tensor([150.5685, 196.8933]), cost: 1.423638\n","Epoch:    6/20, Batch:2/3, hypothesis: tensor([141.5441, 180.2087]), cost: 0.125700\n","Epoch:    6/20, Batch:3/3, hypothesis: 185.0852508544922, cost: 0.007268\n","Epoch:    7/20, Batch:1/3, hypothesis: tensor([196.9481, 180.2017]), cost: 0.469799\n","Epoch:    7/20, Batch:2/3, hypothesis: tensor([184.7382, 150.3580]), cost: 1.382415\n","Epoch:    7/20, Batch:3/3, hypothesis: 141.6171417236328, cost: 0.146580\n","Epoch:    8/20, Batch:1/3, hypothesis: tensor([141.7288, 197.2156]), cost: 0.775578\n","Epoch:    8/20, Batch:2/3, hypothesis: tensor([150.5902, 185.0171]), cost: 0.993944\n","Epoch:    8/20, Batch:3/3, hypothesis: 180.4618377685547, cost: 0.213294\n","Epoch:    9/20, Batch:1/3, hypothesis: tensor([141.5661, 180.2374]), cost: 0.122299\n","Epoch:    9/20, Batch:2/3, hypothesis: tensor([150.6618, 197.0128]), cost: 1.408355\n","Epoch:    9/20, Batch:3/3, hypothesis: 185.10923767089844, cost: 0.011933\n","Epoch:   10/20, Batch:1/3, hypothesis: tensor([150.6222, 180.2141]), cost: 0.972071\n","Epoch:   10/20, Batch:2/3, hypothesis: tensor([141.7261, 197.2126]), cost: 0.772705\n","Epoch:   10/20, Batch:3/3, hypothesis: 185.01510620117188, cost: 0.000228\n","Epoch:   11/20, Batch:1/3, hypothesis: tensor([185.0075, 141.5120]), cost: 0.119104\n","Epoch:   11/20, Batch:2/3, hypothesis: tensor([150.6593, 197.0090]), cost: 1.407718\n","Epoch:   11/20, Batch:3/3, hypothesis: 180.2661590576172, cost: 0.070841\n","Epoch:   12/20, Batch:1/3, hypothesis: tensor([141.4876, 150.5573]), cost: 1.172019\n","Epoch:   12/20, Batch:2/3, hypothesis: tensor([185.3737, 197.3043]), cost: 0.920408\n","Epoch:   12/20, Batch:3/3, hypothesis: 180.09197998046875, cost: 0.008460\n","Epoch:   13/20, Batch:1/3, hypothesis: tensor([196.7794, 180.0473]), cost: 0.304817\n","Epoch:   13/20, Batch:2/3, hypothesis: tensor([150.2980, 141.2496]), cost: 1.729908\n","Epoch:   13/20, Batch:3/3, hypothesis: 185.1610565185547, cost: 0.025939\n","Epoch:   14/20, Batch:1/3, hypothesis: tensor([196.9898, 185.0804]), cost: 0.493043\n","Epoch:   14/20, Batch:2/3, hypothesis: tensor([141.3494, 150.4074]), cost: 1.479737\n","Epoch:   14/20, Batch:3/3, hypothesis: 180.40814208984375, cost: 0.166580\n","Epoch:   15/20, Batch:1/3, hypothesis: tensor([196.9560, 185.0488]), cost: 0.458199\n","Epoch:   15/20, Batch:2/3, hypothesis: tensor([150.3956, 141.3383]), cost: 1.505903\n","Epoch:   15/20, Batch:3/3, hypothesis: 180.39846801757812, cost: 0.158777\n","Epoch:   16/20, Batch:1/3, hypothesis: tensor([150.6154, 141.5394]), cost: 1.064597\n","Epoch:   16/20, Batch:2/3, hypothesis: tensor([180.5756, 185.4197]), cost: 0.253742\n","Epoch:   16/20, Batch:3/3, hypothesis: 197.08926391601562, cost: 1.186496\n","Epoch:   17/20, Batch:1/3, hypothesis: tensor([179.7558, 150.2364]), cost: 1.584990\n","Epoch:   17/20, Batch:2/3, hypothesis: tensor([196.9202, 185.0148]), cost: 0.423462\n","Epoch:   17/20, Batch:3/3, hypothesis: 141.3260498046875, cost: 0.454209\n","Epoch:   18/20, Batch:1/3, hypothesis: tensor([196.9270, 150.5976]), cost: 1.412972\n","Epoch:   18/20, Batch:2/3, hypothesis: tensor([141.5553, 180.2260]), cost: 0.124441\n","Epoch:   18/20, Batch:3/3, hypothesis: 185.09364318847656, cost: 0.008769\n","Epoch:   19/20, Batch:1/3, hypothesis: tensor([196.9542, 150.6190]), cost: 1.408805\n","Epoch:   19/20, Batch:2/3, hypothesis: tensor([185.0778, 180.2395]), cost: 0.031707\n","Epoch:   19/20, Batch:3/3, hypothesis: 141.50555419921875, cost: 0.244477\n","Epoch:   20/20, Batch:1/3, hypothesis: tensor([141.6498, 197.1060]), cost: 0.672993\n","Epoch:   20/20, Batch:2/3, hypothesis: tensor([184.9581, 150.5453]), cost: 1.058992\n","Epoch:   20/20, Batch:3/3, hypothesis: 180.43017578125, cost: 0.185051\n","tensor([[180.2211]], grad_fn=<AddmmBackward>)\n"],"name":"stdout"}]}]}